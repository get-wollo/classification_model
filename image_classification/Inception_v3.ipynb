{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FastText' from 'gensim' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-9d134bd61a9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFastText\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'FastText' from 'gensim' (unknown location)"
     ]
    }
   ],
   "source": [
    "from gensim import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_36_36 = torch.load('cleaned_images/images_36_36.pt')\n",
    "imgs_40_40 = torch.load('cleaned_images/images_40_40.pt')\n",
    "imgs_50_38 = torch.load('cleaned_images/images_50_38.pt')\n",
    "imgs_65_50 = torch.load('cleaned_images/images_65_50.pt')\n",
    "title_vectors_36_36 = pd.read_csv('cleaned_titles/labels_36_36.csv')\n",
    "title_vectors_40_40 = pd.read_csv('cleaned_titles/labels_40_40.csv')\n",
    "title_vectors_50_38 = pd.read_csv('cleaned_titles/labels_50_38.csv')\n",
    "title_vectors_65_50 = pd.read_csv('cleaned_titles/labels_65_50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"JSON_data/data40.json\", 'r') as f:\n",
    "  data_40_json= json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_36_36 = title_vectors_36_36['IntLabel'].astype(int)\n",
    "labels_40_40 = title_vectors_40_40['IntLabel'].astype(int)\n",
    "labels_50_38 = title_vectors_50_38['IntLabel'].astype(int)\n",
    "labels_65_50 = title_vectors_65_50['IntLabel'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 4 artists>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(['36x36','40x40','50x38','65x50'],[labels_36_36.size,labels_40_40.size,labels_50_38.size,labels_65_50.size])\n",
    "#most images arre 40x40 so lets focus on that dataset for now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_40_40_tensor = torch.LongTensor(labels_40_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19, 19, 19,  ...,  3,  3,  3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_40_40_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['[40 40]'], dtype='<U7'), array([338845], dtype=int64))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_size(tensor):\n",
    "  return tensor.size()\n",
    "\n",
    "\n",
    "def array_for(x):\n",
    "    return np.array([get_size(xi) for xi in x])\n",
    "\n",
    "sizes = array_for(imgs_40_40)\n",
    "b = sizes[:,1:3]\n",
    "y = np.array([str(xx) for xx in b])\n",
    "np.unique(y, return_counts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagged_indexes = []\n",
    "imgs_40_40_float = torch.zeros(imgs_40_40.size,3,40,40)\n",
    "for i,x in enumerate(imgs_40_40):\n",
    "  try:\n",
    "    imgs_40_40_float[i] = x.float()\n",
    "  except:\n",
    "    print(f'img looks like: {x}\\n\\n\\n')\n",
    "    print(f'size of image is: {x.size()}\\n\\n\\n\\n\\n')\n",
    "    print(f'index position is {i}')\n",
    "    flagged_indexes.append(i)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\patri/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "c:\\Users\\patri\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\patri\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'inception_v3', pretrained=True)\n",
    "model.train()\n",
    "\n",
    "#https://pytorch.org/hub/pytorch_vision_inception_v3/\n",
    "#disabling training for all parameters\n",
    "for para in model.parameters():\n",
    "  para.requires_grad = False\n",
    "\n",
    "model.aux_logits = False\n",
    "# Only training last layer\n",
    "model.fc = nn.Linear(2048,20)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageWithTitleDataset(torch.utils.data.Dataset):\n",
    "#     \"\"\"Uses jsonl data to preprocess and serve \n",
    "#     dictionary of multimodal tensors for model input.\n",
    "#     find the example of how to use this here: https://drivendata.co/blog/hateful-memes-benchmark/\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         data_path,\n",
    "#         img_dir,\n",
    "#         image_transform,\n",
    "#         text_transform,\n",
    "#         balance=False,\n",
    "#         dev_limit=None,\n",
    "#         random_state=0,\n",
    "#     ):\n",
    "\n",
    "#         self.samples_frame = pd.read_json(\n",
    "#             data_path, lines=True\n",
    "#         )\n",
    "#         self.dev_limit = dev_limit\n",
    "#         if balance:\n",
    "#             neg = self.samples_frame[\n",
    "#                 self.samples_frame.label.eq(0)\n",
    "#             ]\n",
    "#             pos = self.samples_frame[\n",
    "#                 self.samples_frame.label.eq(1)\n",
    "#             ]\n",
    "#             self.samples_frame = pd.concat(\n",
    "#                 [\n",
    "#                     neg.sample(\n",
    "#                         pos.shape[0], \n",
    "#                         random_state=random_state\n",
    "#                     ), \n",
    "#                     pos\n",
    "#                 ]\n",
    "#             )\n",
    "#         if self.dev_limit:\n",
    "#             if self.samples_frame.shape[0] > self.dev_limit:\n",
    "#                 self.samples_frame = self.samples_frame.sample(\n",
    "#                     dev_limit, random_state=random_state\n",
    "#                 )\n",
    "#         self.samples_frame = self.samples_frame.reset_index(\n",
    "#             drop=True\n",
    "#         )\n",
    "#         self.samples_frame.img = self.samples_frame.apply(\n",
    "#             lambda row: (img_dir / row.img), axis=1\n",
    "#         )\n",
    "\n",
    "#         # https://github.com/drivendataorg/pandas-path\n",
    "#         if not self.samples_frame.img.path.exists().all():\n",
    "#             raise FileNotFoundError\n",
    "#         if not self.samples_frame.img.path.is_file().all():\n",
    "#             raise TypeError\n",
    "            \n",
    "#         self.image_transform = image_transform\n",
    "#         self.text_transform = text_transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"This method is called when you do len(instance) \n",
    "#         for an instance of this class.\n",
    "#         \"\"\"\n",
    "#         return len(self.samples_frame)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         \"\"\"This method is called when you do instance[key] \n",
    "#         for an instance of this class.\n",
    "#         \"\"\"\n",
    "#         if torch.is_tensor(idx):\n",
    "#             idx = idx.tolist()\n",
    "\n",
    "#         img_id = self.samples_frame.loc[idx, \"id\"]\n",
    "\n",
    "#         image = Image.open(\n",
    "#             self.samples_frame.loc[idx, \"img\"]\n",
    "#         ).convert(\"RGB\")\n",
    "#         image = self.image_transform(image)\n",
    "\n",
    "#         text = torch.Tensor(\n",
    "#             self.text_transform.get_sentence_vector(\n",
    "#                 self.samples_frame.loc[idx, \"text\"]\n",
    "#             )\n",
    "#         ).squeeze()\n",
    "\n",
    "#         if \"label\" in self.samples_frame.columns:\n",
    "#             label = torch.Tensor(\n",
    "#                 [self.samples_frame.loc[idx, \"label\"]]\n",
    "#             ).long().squeeze()\n",
    "#             sample = {\n",
    "#                 \"id\": img_id, \n",
    "#                 \"image\": image, \n",
    "#                 \"text\": text, \n",
    "#                 \"label\": label\n",
    "#             }\n",
    "#         else:\n",
    "#             sample = {\n",
    "#                 \"id\": img_id, \n",
    "#                 \"image\": image, \n",
    "#                 \"text\": text\n",
    "#             }\n",
    "\n",
    "#         return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LanguageAndVisionConcat(torch.nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         num_classes,\n",
    "#         loss_fn,\n",
    "#         language_module,\n",
    "#         vision_module,\n",
    "#         language_feature_dim,\n",
    "#         vision_feature_dim,\n",
    "#         fusion_output_size,\n",
    "#         dropout_p,\n",
    "        \n",
    "#     ):\n",
    "#         super(LanguageAndVisionConcat, self).__init__()\n",
    "#         self.language_module = language_module\n",
    "#         self.vision_module = vision_module\n",
    "#         self.fusion = torch.nn.Linear(\n",
    "#             in_features=(language_feature_dim + vision_feature_dim), \n",
    "#             out_features=fusion_output_size\n",
    "#         )\n",
    "#         self.fc = torch.nn.Linear(\n",
    "#             in_features=fusion_output_size, \n",
    "#             out_features=num_classes\n",
    "#         )\n",
    "#         self.loss_fn = loss_fn\n",
    "#         self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        \n",
    "#     def forward(self, text, image, label=None):\n",
    "#         text_features = torch.nn.functional.relu(\n",
    "#             self.language_module(text)\n",
    "#         )\n",
    "#         image_features = torch.nn.functional.relu(\n",
    "#             self.vision_module(image)\n",
    "#         )\n",
    "#         combined = torch.cat(\n",
    "#             [text_features, image_features], dim=1\n",
    "#         )\n",
    "#         fused = self.dropout(\n",
    "#             torch.nn.functional.relu(\n",
    "#             self.fusion(combined)\n",
    "#             )\n",
    "#         )\n",
    "#         logits = self.fc(fused)\n",
    "#         pred = torch.nn.functional.softmax(logits)\n",
    "#         loss = (\n",
    "#             self.loss_fn(pred, label) \n",
    "#             if label is not None else label\n",
    "#         )\n",
    "#         return (pred, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(Dataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors, transform=None):\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\n",
    "        self.tensors = tensors\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[0][index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        y = self.tensors[1][index]\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "        \n",
    "def imshow(img, title=''):\n",
    "    \"\"\"Plot the image batch.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title(title)\n",
    "    plt.imshow(np.transpose( img.numpy().astype(np.uint8), (1, 2, 0)), cmap='gray',vmin= -255, vmax= 255)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(299,interpolation = T.InterpolationMode.BILINEAR ),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_tensor = transform(imgs_40_40_float[0])\n",
    "imshow(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataloaders sorted \n",
    "\n",
    "# split the images and convert to dataloaders\n",
    "train_data,test_data, train_labels,test_labels = train_test_split(imgs_40_40_float, labels_40_40_tensor, test_size=.1)\n",
    "\n",
    "# convert into PyTorch Datasets\n",
    "\n",
    "\n",
    "train_data = CustomTensorDataset(tensors=(train_data, train_labels), transform=transform)\n",
    "test_data = CustomTensorDataset(tensors=(test_data, test_labels), transform=transform)\n",
    "\n",
    "\n",
    "# # translate into dataloader objects\n",
    "batchsize = 32\n",
    "letter_train_loader = DataLoader(train_data,batch_size=batchsize,shuffle=True,drop_last=True)\n",
    "letter_test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(letter_train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "  output = model(next(iter(letter_train_loader))[0])\n",
    "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "print(output[0])\n",
    "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#technique to deal with biased dataset\n",
    "\n",
    "def calc_class_weight(classIndex):\n",
    "  return  1 - (sum(labels_40_40_tensor[labels_40_40_tensor == classIndex])/labels_40_40_tensor.size()[0])\n",
    "\n",
    "\n",
    "#class weights for 20 class multi-class classification\n",
    "class_weights = torch.FloatTensor([calc_class_weight(x) for x in labels_40_40_tensor.unique()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#loss function with class weights\n",
    "lossfun = nn.CrossEntropyLoss(weight = class_weights)\n",
    "optimizer = torch.optim.Adam(params = filter(lambda p: p.requires_grad, model.parameters()),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that trains the model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def function2trainTheModel(net,optimizer,train_loader,test_loader,numepochs=1):\n",
    "\n",
    "  # send the model to the GPU\n",
    "  net.to(device)\n",
    "\n",
    "  # initialize losses\n",
    "  trainLoss = torch.zeros(numepochs)\n",
    "  testLoss  = torch.zeros(numepochs)\n",
    "  trainErr  = torch.zeros(numepochs)\n",
    "  testErr   = torch.zeros(numepochs)\n",
    "\n",
    "\n",
    "  # loop over epochs\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    # loop over training data batches\n",
    "    net.train()\n",
    "    batchLoss = []\n",
    "    batchErr  = []\n",
    "    for X,y in train_loader:\n",
    "\n",
    "      # push data to GPU\n",
    "      X = X.to(device)\n",
    "      y = y.to(device)\n",
    "\n",
    "      # forward pass and loss\n",
    "      yHat = net(X)\n",
    "    \n",
    "      loss = lossfun(yHat,y)\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      # loss and error from this batch\n",
    "      batchLoss.append(loss.item())\n",
    "      batchErr.append( torch.mean((torch.argmax(yHat,axis=1) != y).float()).item() )\n",
    "    # end of batch loop...\n",
    "\n",
    "    # and get average losses and error rates across the batches\n",
    "    trainLoss[epochi] = np.mean(batchLoss)\n",
    "    trainErr[epochi]  = 100*np.mean(batchErr)\n",
    "\n",
    "\n",
    "\n",
    "    ### test performance\n",
    "    net.eval()\n",
    "    X,y = next(iter(test_loader)) # extract X,y from test dataloader\n",
    "\n",
    "    # push data to GPU\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad(): # deactivates autograd\n",
    "      yHat = net(X)\n",
    "      loss = lossfun(yHat,y)\n",
    "      \n",
    "    # get loss and error rate from the test batch\n",
    "    testLoss[epochi] = loss.item()\n",
    "    testErr[epochi]  = 100*torch.mean((torch.argmax(yHat,axis=1) != y).float()).item()\n",
    "\n",
    "  # end epochs\n",
    "\n",
    "  # function output\n",
    "  return trainLoss,testLoss,trainErr,testErr,net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1072812 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-c4f2afc30aef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfunction2trainTheModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mletter_train_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mletter_test_loader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnumepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-b35e7cb1dabd>\u001b[0m in \u001b[0;36mfunction2trainTheModel\u001b[1;34m(net, optimizer, train_loader, test_loader, numepochs)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m### test performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# extract X,y from test dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# push data to GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\patri\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    626\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 628\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\patri\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\patri\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\patri\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-967193cd73e6>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\patri\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\patri\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\patri\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    268\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \"\"\"\n\u001b[1;32m--> 270\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\patri\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\patri\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 929\u001b[1;33m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1072812 bytes."
     ]
    }
   ],
   "source": [
    "function2trainTheModel(model,optimizer,letter_train_loader,letter_test_loader,numepochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eb5d0a65b500759bcde1c4c1ad0551eaece71d5bef76353acf57400c52edb49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
